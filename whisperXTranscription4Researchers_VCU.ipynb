{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0.0 WhisperX/Pyannote Transcription+Diarization Pipeline \n",
    "\n",
    "This Jupyter notebook is designed to test and evaluate a new Transcription and Diarization Pipeline with the following objectives:\n",
    "1. Achieving word-level transcription accuracy to ensure detailed and precise text representation of the audio input.\n",
    "2. Assessing diarization confidence levels to accurately attribute spoken segments to different speakers and measure the reliability of speaker identification.\n",
    "3. Enhancing the alignment of transcriptions to be closer to natural sentence segments, thereby improving the readability and usability of the transcribed data.\n",
    "\n",
    "The notebook leverages advanced transcription and diarization capabilities provided by the Whisper, WhisperX, and pyannote libraries. By using GPU acceleration, it processes audio data efficiently, performing alignment and diarization to produce structured outputs that are saved in CSV format for further analysis. The resources and installation instructions are included to facilitate the setup and execution of the pipeline.\n",
    "\n",
    "Resources:\n",
    "https://towardsdatascience.com/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Setup\n",
    "WhisperX documentation found here: https://github.com/m-bain/whisperX\n",
    "================================================\n",
    "1. Install Git\n",
    "2. Install FFMPEG and add to PATH\n",
    "3. Install Anaconda \n",
    "\n",
    "================================================   \n",
    "4. Create Conda environment\n",
    "```sh\n",
    "conda create -n whisperxtranscription-env python=3.10\n",
    "conda activate whisperxtranscription-env\n",
    "```\n",
    "5. Install PyTorch https://pytorch.org/get-started/locally/ \n",
    "```sh\n",
    "pip install numpy==1.26.3 torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "```\n",
    "6. Install WhisperX repository and additional packages\n",
    "```sh\n",
    "pip install whisperx==3.2.0\n",
    "\n",
    "pip install speechbrain ipykernel ipywidgets charset-normalizer pandas nltk plotly matplotlib webvtt-py pypi-json srt python-dotenv tqdm\n",
    "\n",
    "```\n",
    "\n",
    "7. Create .env file at the same level as this notebook file with the following line\n",
    "```sh\n",
    "HF_TOKEN=\"REPLACEWITHHUGGINGFACETOKENHERE\"\n",
    "```\n",
    "=================================================\n",
    "8. For GPU usage :\n",
    "Install Visual Studio Community https://visualstudio.microsoft.com/downloads/\n",
    "Install NVIDIA CUDA Toolkit 12.1 https://developer.nvidia.com/cuda-12-1-0-download-archive \n",
    "\n",
    "Check PyTorch and CUDA installation\n",
    "```sh\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "```\n",
    "\n",
    "=================================================\n",
    "Fix Numpy\n",
    "```sh\n",
    "pip uninstall numpy -y\n",
    "pip install numpy==1.26.3\n",
    "```\n",
    "\n",
    "Fix PyTorch\n",
    "```sh\n",
    "pip uninstall torch torchvision torchaudio -y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1514, 0.7911, 0.6091],\n",
      "        [0.8781, 0.7379, 0.6201],\n",
      "        [0.5455, 0.7667, 0.7895],\n",
      "        [0.7106, 0.6113, 0.1373],\n",
      "        [0.6705, 0.0542, 0.8046]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n",
      "False\n",
      "PyTorch is running on: CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())  # This will return False on Mac\n",
    "print(\"PyTorch is running on:\", \"CPU\" if not torch.cuda.is_available() else \"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 Check once to see if CUDA GPU is available and PyTorch is working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#torch.cuda.set_device(0)                                    # Set the main GPU as device to use if present\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m----> 5\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available(),\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m      \u001b[38;5;66;03m# Check if GPU is available and get the name of the GPU\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/torch/cuda/__init__.py:582\u001b[0m, in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_device_name\u001b[39m(device: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the name of a device.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/torch/cuda/__init__.py:614\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_device_properties\u001b[39m(device: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    603\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \n\u001b[1;32m    605\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 614\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    615\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[0;32m/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/torch/cuda/__init__.py:403\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    401\u001b[0m     )\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# Check if CUDA GPU or Apple MPS is available to PyTorch\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        # Set device to GPU 0 if CUDA is available\n",
    "        torch.cuda.set_device(0)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        print('CUDA available:', torch.cuda.get_device_name(0))\n",
    "    except Exception:\n",
    "        print('CUDA available: (device name unavailable)')\n",
    "elif getattr(torch.backends, 'mps', None) is not None and torch.backends.mps.is_available():\n",
    "    # Apple Silicon (MPS) path\n",
    "    print('MPS backend available (Apple Silicon GPU)')\n",
    "else:\n",
    "    print('No CUDA or MPS available; using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: whisperx\n",
      "Version: 3.7.2\n",
      "Summary: Time-Accurate Automatic Speech Recognition using Whisper.\n",
      "Home-page: \n",
      "Author: Max Bain\n",
      "Author-email: \n",
      "License: BSD-2-Clause\n",
      "Location: /opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages\n",
      "Requires: av, ctranslate2, faster-whisper, nltk, numpy, pandas, pyannote-audio, torch, torchaudio, transformers\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show whisperx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU for processing\n",
      "Device: cpu, Compute type: float32\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Cross-platform compatible\n",
    "# Note: This cell requires the imports from the previous cell to be executed first\n",
    "# Detect appropriate device: CUDA (NVIDIA), MPS (Apple Silicon) or CPU\n",
    "import os\n",
    "import torch\n",
    "\n",
    "device = None\n",
    "compute_type = None\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        torch.cuda.set_device(0)\n",
    "    except Exception:\n",
    "        pass\n",
    "    device = 'cuda'\n",
    "    compute_type = 'float16'\n",
    "    try:\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        print(f\"Using device: {device_name} (CUDA)\")\n",
    "    except Exception:\n",
    "        print('Using device: CUDA (name unavailable)')\n",
    "elif getattr(torch.backends, 'mps', None) is not None and torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    compute_type = 'float32'\n",
    "    print('Using device: MPS (Apple Silicon)')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    compute_type = 'float32'\n",
    "    print('Using device: CPU')\n",
    "\n",
    "language = 'en'  # Set the language code en=English, es=Spanish, etc.\n",
    "task = 'transcribe'  # Set the task to 'transcribe' or 'translate'\n",
    "batch_size = 16 # Set the batch size for processing\n",
    "hf_token = os.getenv('HF_TOKEN')  # Requires 'os' from previous cell\n",
    "whisperx_model = 'large-v3' # Set the WhisperX model to use\n",
    "extensions = ['.ogg', '.wav', '.mp3'] # Supported audio file extensions\n",
    "\n",
    "print(f\"Device: {device}, Compute type: {compute_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Setup - Start here by adjusting variables\n",
    "1. choose batch size, compute type, whisper model, and file extension to transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch._C' has no attribute '_cuda_setDevice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m     20\u001b[0m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeechbrain.utils.quirks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mWARNING)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Configuration\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Set GPU\u001b[39;00m\n\u001b[1;32m     26\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Set device to GPU if available, otherwise CPU\u001b[39;00m\n\u001b[1;32m     27\u001b[0m language \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Set the language code en=English, es=Spanish, etc.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/torch/cuda/__init__.py:567\u001b[0m, in \u001b[0;36mset_device\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    565\u001b[0m device \u001b[38;5;241m=\u001b[39m _get_device_index(device)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_setDevice\u001b[49m(device)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch._C' has no attribute '_cuda_setDevice'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tkinter import Tk, filedialog\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import torch\n",
    "import whisperx\n",
    "import gc\n",
    "import datetime\n",
    "import json\n",
    "import webvtt\n",
    "import logging\n",
    "import dotenv\n",
    "\n",
    "# Suppress specific deprecation warnings from torchaudio and speechbrain\n",
    "warnings.filterwarnings(\"ignore\", message=\".*set_audio_backend has been deprecated.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*get_audio_backend has been deprecated.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Module 'speechbrain.pretrained' was deprecated.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*AudioMetaData.*moved to.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger(\"speechbrain.utils.quirks\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "# Configuration\n",
    "# Detect appropriate device for this environment (CUDA, MPS, or CPU)\n",
    "device = None\n",
    "compute_type = None\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        torch.cuda.set_device(0)\n",
    "    except Exception:\n",
    "        pass\n",
    "    device = \"cuda\"\n",
    "    compute_type = \"float16\"\n",
    "elif getattr(torch.backends, 'mps', None) is not None and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    compute_type = \"float32\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    compute_type = \"float32\"\n",
    "\n",
    "language = \"en\"  # Set the language code en=English, es=Spanish, etc.\n",
    "task = \"transcribe\"  # Set the task to \"transcribe\" or \"translate\" \n",
    "batch_size = 16 # Set the batch size for processing\n",
    "hf_token = os.getenv('HF_TOKEN') \n",
    "whisperx_model = \"large-v3\" # Set the WhisperX model to use\n",
    "extensions = ['.ogg', '.wav', '.mp3'] # Supported audio file extensions\n",
    "\n",
    "print(f\"Device: {device}, Compute type: {compute_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Run - after adjusting variables first\n",
    "\n",
    "Just push run here. You shouldn't need to change anything here unless you want to output less or more file types. These are mostly functions which are then called at the end of the cell.\n",
    "\n",
    "1. You should get a popup asking to choose the folder where the files are found (It will also search subfolders).\n",
    "\n",
    "2. You should then get a popup asking for where the transcription files should be placed (It will replicate the folder structure in which they were found)\n",
    "\n",
    "3. You will also see a popup asking if you want to anonymize with a pseudonyms.csv file, and if so where it is located.\n",
    "\n",
    "4. You should then see an output similar to the following (just ignore the warnings):\n",
    "\n",
    "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
    "Model was trained with torch 1.10.0+cu102, yours is 2.3.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
    "\n",
    "5. When complete you will see where each were written and the folders where they were written to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mac-compatible version - Run this cell instead of the previous one\n",
    "# This avoids tkinter issues that cause Python to hang on Mac\n",
    "\n",
    "def main_mac_compatible():\n",
    "    print(\"=== WhisperX Transcription Pipeline (Mac Compatible) ===\")\n",
    "    print(\"Please provide the following paths:\")\n",
    "    print()\n",
    "    \n",
    "    # Get input folder path\n",
    "    print(\"1. Input folder containing audio/video files:\")\n",
    "    print(\"   (You can drag and drop a folder from Finder into Terminal)\")\n",
    "    input_folder = input(\"   Enter the full path: \").strip().strip(\"'\\\"\")\n",
    "    \n",
    "    if not input_folder or not os.path.exists(input_folder):\n",
    "        print(\"Invalid input folder path. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"   ✓ Using input folder: {input_folder}\")\n",
    "    print()\n",
    "    \n",
    "    # Get output folder path\n",
    "    print(\"2. Output folder for transcriptions:\")\n",
    "    print(\"   (You can drag and drop a folder from Finder into Terminal)\")\n",
    "    output_folder = input(\"   Enter the full path: \").strip().strip(\"'\\\"\")\n",
    "    \n",
    "    if not output_folder:\n",
    "        print(\"No output folder specified. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"   ✓ Using output folder: {output_folder}\")\n",
    "    print()\n",
    "    \n",
    "    # Ask about pseudonyms\n",
    "    print(\"3. Pseudonymization:\")\n",
    "    use_pseudonyms = input(\"   Do you want to use a pseudonyms.csv file? (y/n): \").strip().lower()\n",
    "    pseudonym_dict = None\n",
    "    \n",
    "    if use_pseudonyms in ['y', 'yes']:\n",
    "        print(\"   Enter the full path to your pseudonyms.csv file:\")\n",
    "        pseudonyms_file = input(\"   Path: \").strip().strip(\"'\\\"\")\n",
    "        \n",
    "        if pseudonyms_file and os.path.exists(pseudonyms_file):\n",
    "            try:\n",
    "                pseudonyms_df = pd.read_csv(pseudonyms_file)\n",
    "                pseudonym_dict = dict(zip(pseudonyms_df['name'], pseudonyms_df['pseudonym']))\n",
    "                print(f\"   ✓ Pseudonyms loaded from {pseudonyms_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠ Error loading pseudonyms file: {e}\")\n",
    "                print(\"   Continuing without pseudonymization.\")\n",
    "        else:\n",
    "            print(\"   ⚠ Pseudonyms file not found. Continuing without pseudonymization.\")\n",
    "    \n",
    "    print()\n",
    "    print(\"=== Starting Processing ===\")\n",
    "    \n",
    "    # Find and process audio files\n",
    "    audio_files = find_audio_files(input_folder, extensions)\n",
    "    print(f\"Found {len(audio_files)} files to process.\")\n",
    "    \n",
    "    if len(audio_files) == 0:\n",
    "        print(\"No audio files found in the specified folder.\")\n",
    "        return\n",
    "    \n",
    "    for audio_file in audio_files:\n",
    "        relative_path = os.path.relpath(audio_file, input_folder)\n",
    "        process_audio_file(audio_file, output_folder, relative_path, pseudonym_dict)\n",
    "        print(f\"Processed {audio_file}\")\n",
    "    \n",
    "    print(\"All files processed.\")\n",
    "\n",
    "# Run the Mac-compatible version\n",
    "main_mac_compatible()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 files to process.\n",
      "Processing /Users/kevinhall/Documents/AANLP_Work/WhisperXTranscription4Researchers_VibeCodingUpdate/Data/rawAudioFiles/Monologue1.ogg...\n",
      "Error processing /Users/kevinhall/Documents/AANLP_Work/WhisperXTranscription4Researchers_VibeCodingUpdate/Data/rawAudioFiles/Monologue1.ogg:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2317, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2347, in _get_module\n",
      "    raise e\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2345, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/transformers/pipelines/__init__.py\", line 26, in <module>\n",
      "    from ..image_processing_utils import BaseImageProcessor\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/transformers/image_processing_utils.py\", line 21, in <module>\n",
      "    from .image_processing_base import BatchFeature, ImageProcessingMixin\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/transformers/image_processing_base.py\", line 26, in <module>\n",
      "    from .image_utils import is_valid_image, load_image\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/transformers/image_utils.py\", line 55, in <module>\n",
      "    from torchvision.transforms import InterpolationMode\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/torchvision/__init__.py\", line 6, in <module>\n",
      "    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/torchvision/_meta_registrations.py\", line 164, in <module>\n",
      "    def meta_nms(dets, scores, iou_threshold):\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/torch/library.py\", line 1063, in register\n",
      "    use_lib._register_fake(\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/torch/library.py\", line 211, in _register_fake\n",
      "    handle = entry.fake_impl.register(\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/torch/_library/fake_impl.py\", line 50, in register\n",
      "    if torch._C._dispatch_has_kernel_for_dispatch_key(self.qualname, \"Meta\"):\n",
      "RuntimeError: operator torchvision::nms does not exist\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/19/6pjx738x259c707gbz3_6cd80000gn/T/ipykernel_94554/1870150986.py\", line 60, in process_audio_file\n",
      "    model = whisperx.load_model(whisperx_model, device, compute_type=compute_type)\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/whisperx/__init__.py\", line 20, in load_model\n",
      "    asr = _lazy_import(\"asr\")\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/whisperx/__init__.py\", line 5, in _lazy_import\n",
      "    module = importlib.import_module(f\"whisperx.{name}\")\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/whisperx/asr.py\", line 11, in <module>\n",
      "    from transformers import Pipeline\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2320, in __getattr__\n",
      "    raise ModuleNotFoundError(\n",
      "ModuleNotFoundError: Could not import module 'Pipeline'. Are this object's requirements defined correctly?\n",
      "\n",
      "Processed /Users/kevinhall/Documents/AANLP_Work/WhisperXTranscription4Researchers_VibeCodingUpdate/Data/rawAudioFiles/Monologue1.ogg\n",
      "Processing /Users/kevinhall/Documents/AANLP_Work/WhisperXTranscription4Researchers_VibeCodingUpdate/Data/rawAudioFiles/Monologue2.ogg...\n",
      "Error processing /Users/kevinhall/Documents/AANLP_Work/WhisperXTranscription4Researchers_VibeCodingUpdate/Data/rawAudioFiles/Monologue2.ogg:\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/19/6pjx738x259c707gbz3_6cd80000gn/T/ipykernel_94554/1870150986.py\", line 60, in process_audio_file\n",
      "    model = whisperx.load_model(whisperx_model, device, compute_type=compute_type)\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/whisperx/__init__.py\", line 20, in load_model\n",
      "    asr = _lazy_import(\"asr\")\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/whisperx/__init__.py\", line 5, in _lazy_import\n",
      "    module = importlib.import_module(f\"whisperx.{name}\")\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/whisperx/asr.py\", line 11, in <module>\n",
      "    from transformers import Pipeline\n",
      "ImportError: cannot import name 'Pipeline' from 'transformers' (/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/transformers/__init__.py)\n",
      "\n",
      "Processed /Users/kevinhall/Documents/AANLP_Work/WhisperXTranscription4Researchers_VibeCodingUpdate/Data/rawAudioFiles/Monologue2.ogg\n",
      "All files processed.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tkinter import Tk, filedialog, messagebox\n",
    "\n",
    "# Functions\n",
    "def find_audio_files(base_dir, extensions):\n",
    "    audio_files = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext.lower()) for ext in extensions):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    return audio_files\n",
    "\n",
    "def anonymize_text(text, pseudonym_dict):\n",
    "    for real_name, pseudonym in (pseudonym_dict or {}).items():\n",
    "        text = text.replace(real_name, pseudonym)\n",
    "    return text\n",
    "\n",
    "def format_vtt_timestamp(seconds):\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    milliseconds = int((seconds % 1) * 1000)\n",
    "    return f\"{int(hours):02}:{int(minutes):02}:{int(seconds):02}.{milliseconds:03}\"\n",
    "\n",
    "def save_transcripts(segments, output_dir, relative_path, pseudonym_dict=None):\n",
    "    if pseudonym_dict:\n",
    "        for segment in segments:\n",
    "            segment['text'] = anonymize_text(segment['text'], pseudonym_dict)\n",
    "    for i, segment in enumerate(segments):\n",
    "        segment['sentence_number'] = i + 1\n",
    "    df = pd.DataFrame(segments)\n",
    "    df['text'] = df['text'].apply(lambda x: x.lstrip())\n",
    "    cols = ['sentence_number'] + [col for col in df.columns if col != 'sentence_number']\n",
    "    df = df[cols]\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    base_filename = os.path.splitext(os.path.basename(relative_path))[0]\n",
    "    csv_path = os.path.join(output_dir, f\"{base_filename}_transcription.csv\")\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    with open(os.path.join(output_dir, f\"{base_filename}_transcription.txt\"), 'w', encoding='utf-8') as f:\n",
    "        for segment in segments:\n",
    "            f.write(f\"{segment['text'].strip()}\\n\")\n",
    "\n",
    "    json_path = os.path.join(output_dir, f\"{base_filename}_transcription.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(segments, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    vtt = webvtt.WebVTT()\n",
    "    for segment in segments:\n",
    "        caption = webvtt.Caption()\n",
    "        caption.start = format_vtt_timestamp(segment['start'])\n",
    "        caption.end = format_vtt_timestamp(segment['end'])\n",
    "        caption.lines = [f\"{segment['sentence_number']}: {segment['text'].strip()}\"]\n",
    "        vtt.captions.append(caption)\n",
    "    vtt.save(os.path.join(output_dir, f\"{base_filename}_transcription.vtt\"))\n",
    "\n",
    "def process_audio_file(audio_file, base_output_dir, relative_path, pseudonym_dict=None):\n",
    "    try:\n",
    "        print(f\"Processing {audio_file}...\")\n",
    "        audio = whisperx.load_audio(audio_file)\n",
    "        model = whisperx.load_model(whisperx_model, device, compute_type=compute_type)\n",
    "        result = model.transcribe(audio, batch_size=batch_size, language=language, task=task)\n",
    "        # Clean up model and free CUDA memory only when CUDA is present\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                torch.cuda.empty_cache()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        model_a, metadata = whisperx.load_align_model(language_code=language, device=device)\n",
    "        result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "        del model_a\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                torch.cuda.empty_cache()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Correct way to load diarization model in recent whisperx\n",
    "        diarize_model = whisperx.DiarizationPipeline(use_auth_token=hf_token, device=device)\n",
    "        diarize_segments = diarize_model(audio)\n",
    "        result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "\n",
    "        output_dir = os.path.join(base_output_dir, os.path.dirname(relative_path))\n",
    "        save_transcripts(result[\"segments\"], output_dir, relative_path, pseudonym_dict)\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error processing {audio_file}:\\n{traceback.format_exc()}\")\n",
    "\n",
    "def main():\n",
    "    # Initialize Tkinter\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "    \n",
    "    # Bring the root window to the front\n",
    "    root.attributes('-topmost', True)\n",
    "\n",
    "    # Popup for input folder\n",
    "    input_folder = filedialog.askdirectory(title=\"Select Folder Containing Audio/Video Files\")\n",
    "    if not input_folder:\n",
    "        print(\"No folder selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Popup for output folder\n",
    "    output_folder = filedialog.askdirectory(title=\"Select Folder to Save Transcriptions\")\n",
    "    if not output_folder:\n",
    "        print(\"No output folder selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Ask if a pseudonyms.csv file will be used\n",
    "    use_pseudonyms = messagebox.askyesno(\"Pseudonyms\", \"Will you use a pseudonyms.csv file for to anonymize the transcripts?\")\n",
    "    pseudonym_dict = None\n",
    "\n",
    "    if use_pseudonyms:\n",
    "        pseudonyms_file = filedialog.askopenfilename(\n",
    "            title=\"Select Pseudonyms CSV File\",\n",
    "            filetypes=[(\"CSV files\", \"*.csv\")]\n",
    "        )\n",
    "        if not pseudonyms_file:\n",
    "            print(\"No pseudonyms file selected. Continuing without pseudonymization.\")\n",
    "        else:\n",
    "            # Load the pseudonyms file\n",
    "            pseudonyms_df = pd.read_csv(pseudonyms_file)\n",
    "            pseudonym_dict = dict(zip(pseudonyms_df['name'], pseudonyms_df['pseudonym']))\n",
    "            print(f\"Pseudonyms loaded from {pseudonyms_file}.\")\n",
    "\n",
    "    # Find and process audio files\n",
    "    audio_files = find_audio_files(input_folder, extensions)\n",
    "    print(f\"Found {len(audio_files)} files to process.\")\n",
    "\n",
    "    for audio_file in audio_files:\n",
    "        relative_path = os.path.relpath(audio_file, input_folder)\n",
    "        process_audio_file(audio_file, output_folder, relative_path, pseudonym_dict)\n",
    "        print(f\"Processed {audio_file}\")\n",
    "\n",
    "    print(\"All files processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## macOS install notes\n",
    "\n",
    "Below are quick install commands you can run from a macOS terminal (or in a notebook cell using `!` to run shell commands). These cover ffmpeg, creating a virtual environment, installing a CPU PyTorch wheel (safe), installing WhisperX and other Python dependencies, and a short verification command. If you want an MPS-enabled wheel for Apple Silicon, visit https://pytorch.org/get-started/locally/ and use the selector to get the recommended command for your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macOS install commands (run in terminal or as notebook shell commands)\n",
    "# 1) Install ffmpeg (Homebrew)\n",
    "!brew install ffmpeg\n",
    "\n",
    "# 2) Create & activate venv (do this in terminal)\n",
    "# python3 -m venv venv\n",
    "# source venv/bin/activate\n",
    "\n",
    "# 3) Upgrade pip and install CPU-only PyTorch wheel (safe fallback)\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# 4) Install WhisperX and dependencies\n",
    "python3 -m pip install whisperx==3.2.0\n",
    "python3 -m pip install speechbrain ipykernel ipywidgets charset-normalizer pandas nltk plotly matplotlib webvtt-py pypi-json srt python-dotenv tqdm\n",
    "\n",
    "# 5) Verify torch and device\n",
    "python3 -c \"import torch; print(torch.__version__); print('MPS', getattr(torch.backends,'mps',None) is not None and torch.backends.mps.is_available()); print('CUDA', torch.cuda.is_available())\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperxtranscription-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
