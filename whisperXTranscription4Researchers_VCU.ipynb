{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0.0 WhisperX/Pyannote Transcription+Diarization Pipeline \n",
    "\n",
    "This Jupyter notebook is designed to test and evaluate a new Transcription and Diarization Pipeline with the following objectives:\n",
    "1. Achieving word-level transcription accuracy to ensure detailed and precise text representation of the audio input.\n",
    "2. Assessing diarization confidence levels to accurately attribute spoken segments to different speakers and measure the reliability of speaker identification.\n",
    "3. Enhancing the alignment of transcriptions to be closer to natural sentence segments, thereby improving the readability and usability of the transcribed data.\n",
    "\n",
    "The notebook leverages advanced transcription and diarization capabilities provided by the Whisper, WhisperX, and pyannote libraries. By using GPU acceleration, it processes audio data efficiently, performing alignment and diarization to produce structured outputs that are saved in CSV format for further analysis. The resources and installation instructions are included to facilitate the setup and execution of the pipeline.\n",
    "\n",
    "Resources:\n",
    "https://towardsdatascience.com/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Setup\n",
    "WhisperX documentation found here: https://github.com/m-bain/whisperX\n",
    "================================================\n",
    "1. Install Git\n",
    "2. Install FFMPEG and add to PATH\n",
    "3. Install Anaconda \n",
    "\n",
    "================================================   \n",
    "4. Create Conda environment\n",
    "```sh\n",
    "conda create -n whisperxtranscription-env python=3.10\n",
    "conda activate whisperxtranscription-env\n",
    "```\n",
    "5. Install PyTorch https://pytorch.org/get-started/locally/ \n",
    "```sh\n",
    "pip install numpy==1.26.3 torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "```\n",
    "6. Install WhisperX repository and additional packages\n",
    "```sh\n",
    "pip install whisperx==3.2.0\n",
    "\n",
    "pip install speechbrain ipykernel ipywidgets charset-normalizer pandas nltk plotly matplotlib webvtt-py pypi-json srt python-dotenv tqdm\n",
    "\n",
    "```\n",
    "\n",
    "7. Create .env file at the same level as this notebook file with the following line\n",
    "```sh\n",
    "HF_TOKEN=\"REPLACEWITHHUGGINGFACETOKENHERE\"\n",
    "```\n",
    "=================================================\n",
    "8. For GPU usage :\n",
    "Install Visual Studio Community https://visualstudio.microsoft.com/downloads/\n",
    "Install NVIDIA CUDA Toolkit 12.1 https://developer.nvidia.com/cuda-12-1-0-download-archive \n",
    "\n",
    "Check PyTorch and CUDA installation\n",
    "```sh\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "```\n",
    "\n",
    "=================================================\n",
    "Fix Numpy\n",
    "```sh\n",
    "pip uninstall numpy -y\n",
    "pip install numpy==1.26.3\n",
    "```\n",
    "\n",
    "Fix PyTorch\n",
    "```sh\n",
    "pip uninstall torch torchvision torchaudio -y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1514, 0.7911, 0.6091],\n",
      "        [0.8781, 0.7379, 0.6201],\n",
      "        [0.5455, 0.7667, 0.7895],\n",
      "        [0.7106, 0.6113, 0.1373],\n",
      "        [0.6705, 0.0542, 0.8046]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 Check once to see if CUDA GPU is available and PyTorch is working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#torch.cuda.set_device(0)                                    # Set the main GPU as device to use if present\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m----> 5\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available(),\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m      \u001b[38;5;66;03m# Check if GPU is available and get the name of the GPU\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/torch/cuda/__init__.py:414\u001b[0m, in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    410\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_init()\n\u001b[1;32m    411\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    416\u001b[0m _queued_calls\u001b[38;5;241m.\u001b[39mextend(calls \u001b[38;5;28;01mfor\u001b[39;00m calls \u001b[38;5;129;01min\u001b[39;00m _lazy_seed_tracker\u001b[38;5;241m.\u001b[39mget_calls() \u001b[38;5;28;01mif\u001b[39;00m calls)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/torch/cuda/__init__.py:444\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcudart\u001b[39m():\n\u001b[1;32m    434\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves the CUDA runtime API module.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \n\u001b[1;32m    436\u001b[0m \n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03m    This function initializes the CUDA runtime environment if it is not already\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03m    initialized and returns the CUDA runtime API module (_cudart). The CUDA\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m    runtime API module provides access to various CUDA runtime functions.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m        ``None``\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \n\u001b[0;32m--> 444\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m        module: The CUDA runtime API module (_cudart).\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m    Raises:\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m        RuntimeError: If CUDA cannot be re-initialized in a forked subprocess.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m        AssertionError: If PyTorch is not compiled with CUDA support or if libcudart functions are unavailable.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03m    Example of CUDA operations with profiling:\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03m        >>> import torch\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;124;03m        >>> from torch.cuda import cudart, check_error\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;124;03m        >>> import os\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;124;03m        >>> os.environ[\"CUDA_PROFILE\"] = \"1\"\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m        >>> def perform_cuda_operations_with_streams():\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;124;03m        >>>     stream = torch.cuda.Stream()\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03m        >>>     with torch.cuda.stream(stream):\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;124;03m        >>>         x = torch.randn(100, 100, device='cuda')\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;124;03m        >>>         y = torch.randn(100, 100, device='cuda')\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;124;03m        >>>         z = torch.mul(x, y)\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m        >>>     return z\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m        >>> torch.cuda.synchronize()\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;124;03m        >>> print(\"====== Start nsys profiling ======\")\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03m        >>> check_error(cudart().cudaProfilerStart())\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m        >>> with torch.autograd.profiler.emit_nvtx():\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03m        >>>     result = perform_cuda_operations_with_streams()\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03m        >>>     print(\"CUDA operations completed.\")\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m        >>> check_error(torch.cuda.cudart().cudaProfilerStop())\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m        >>> print(\"====== End nsys profiling ======\")\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \n\u001b[1;32m    475\u001b[0m \u001b[38;5;124;03m    To run this example and save the profiling information, execute:\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;124;03m        >>> $ nvprof --profile-from-start off --csv --print-summary -o trace_name.prof -f -- python cudart_test.py\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \n\u001b[1;32m    478\u001b[0m \u001b[38;5;124;03m    This command profiles the CUDA operations in the provided script and saves\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;124;03m    the profiling information to a file named `trace_name.prof`.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m    The `--profile-from-start off` option ensures that profiling starts only\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m    after the `cudaProfilerStart` call in the script.\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03m    The `--csv` and `--print-summary` options format the profiling output as a\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m    CSV file and print a summary, respectively.\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;124;03m    The `-o` option specifies the output file name, and the `-f` option forces the\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;124;03m    overwrite of the output file if it already exists.\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    487\u001b[0m     _lazy_init()\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cudart\n",
      "File \u001b[0;32m/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/torch/cuda/__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    278\u001b[0m max_arch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    279\u001b[0m     (_extract_arch_version(arch) \u001b[38;5;28;01mfor\u001b[39;00m arch \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_arch_list()),\n\u001b[1;32m    280\u001b[0m     default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m    281\u001b[0m )\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_arch \u001b[38;5;241m<\u001b[39m min_arch \u001b[38;5;129;01mor\u001b[39;00m current_arch \u001b[38;5;241m>\u001b[39m max_arch:\n\u001b[1;32m    283\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m--> 284\u001b[0m         incompatible_gpu_warn\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    286\u001b[0m             d,\n\u001b[1;32m    287\u001b[0m             name,\n\u001b[1;32m    288\u001b[0m             major,\n\u001b[1;32m    289\u001b[0m             minor,\n\u001b[1;32m    290\u001b[0m             min_arch \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m    291\u001b[0m             min_arch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m    292\u001b[0m             max_arch \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m    293\u001b[0m             max_arch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m    294\u001b[0m         )\n\u001b[1;32m    295\u001b[0m     )\n\u001b[1;32m    296\u001b[0m     matched_arches \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arch, arch_info \u001b[38;5;129;01min\u001b[39;00m CUDA_ARCHES_SUPPORTED\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# Check if CUDA GPU is available to PyTorch\n",
    "import torch                                                # PyTorch\n",
    "#torch.cuda.set_device(0)                                    # Set the main GPU as device to use if present\n",
    "print(torch.__version__)\n",
    "torch.cuda.is_available(),torch.cuda.get_device_name()      # Check if GPU is available and get the name of the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: whisperx\n",
      "Version: 3.7.2\n",
      "Summary: Time-Accurate Automatic Speech Recognition using Whisper.\n",
      "Home-page: \n",
      "Author: Max Bain\n",
      "Author-email: \n",
      "License: BSD-2-Clause\n",
      "Location: /opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages\n",
      "Requires: av, ctranslate2, faster-whisper, nltk, numpy, pandas, pyannote-audio, torch, torchaudio, transformers\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show whisperx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.3\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Setup - Start here by adjusting variables\n",
    "1. choose batch size, compute type, whisper model, and file extension to transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch._C' has no attribute '_cuda_setDevice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m\n\u001b[1;32m     20\u001b[0m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeechbrain.utils.quirks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mWARNING)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Configuration\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Set GPU\u001b[39;00m\n\u001b[1;32m     26\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Set device to GPU if available, otherwise CPU\u001b[39;00m\n\u001b[1;32m     27\u001b[0m language \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Set the language code en=English, es=Spanish, etc.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/whisperxtranscription-env/lib/python3.10/site-packages/torch/cuda/__init__.py:399\u001b[0m, in \u001b[0;36mset_device\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    397\u001b[0m device \u001b[38;5;241m=\u001b[39m _get_device_index(device)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_setDevice\u001b[49m(device)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch._C' has no attribute '_cuda_setDevice'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tkinter import Tk, filedialog\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import torch\n",
    "import whisperx\n",
    "import gc\n",
    "import datetime\n",
    "import json\n",
    "import webvtt\n",
    "import logging\n",
    "import dotenv\n",
    "\n",
    "# Suppress specific deprecation warnings from torchaudio and speechbrain\n",
    "warnings.filterwarnings(\"ignore\", message=\".*set_audio_backend has been deprecated.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*get_audio_backend has been deprecated.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Module 'speechbrain.pretrained' was deprecated.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*AudioMetaData.*moved to.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger(\"speechbrain.utils.quirks\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "# Configuration\n",
    "torch.cuda.set_device(0)  # Set GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Set device to GPU if available, otherwise CPU\n",
    "language = \"en\"  # Set the language code en=English, es=Spanish, etc.\n",
    "task = \"transcribe\"  # Set the task to \"transcribe\" or \"translate\" \n",
    "batch_size = 16 # Set the batch size for processing\n",
    "compute_type = \"float16\" # Set the compute type to \"float16\" for faster processing\n",
    "hf_token = os.getenv('HF_TOKEN') \n",
    "whisperx_model = \"large-v3\" # Set the WhisperX model to use\n",
    "extensions = ['.ogg', '.wav', '.mp3'] # Supported audio file extensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Run - after adjusting variables first\n",
    "\n",
    "Just push run here. You shouldn't need to change anything here unless you want to output less or more file types. These are mostly functions which are then called at the end of the cell.\n",
    "\n",
    "1. You should get a popup asking to choose the folder where the files are found (It will also search subfolders).\n",
    "\n",
    "2. You should then get a popup asking for where the transcription files should be placed (It will replicate the folder structure in which they were found)\n",
    "\n",
    "3. You will also see a popup asking if you want to anonymize with a pseudonyms.csv file, and if so where it is located.\n",
    "\n",
    "4. You should then see an output similar to the following (just ignore the warnings):\n",
    "\n",
    "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
    "Model was trained with torch 1.10.0+cu102, yours is 2.3.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
    "\n",
    "5. When complete you will see where each were written and the folders where they were written to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import Tk, filedialog, messagebox\n",
    "\n",
    "# Functions\n",
    "def find_audio_files(base_dir, extensions):\n",
    "    audio_files = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext.lower()) for ext in extensions):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    return audio_files\n",
    "\n",
    "def anonymize_text(text, pseudonym_dict):\n",
    "    for real_name, pseudonym in pseudonym_dict.items():\n",
    "        text = text.replace(real_name, pseudonym)\n",
    "    return text\n",
    "\n",
    "def format_vtt_timestamp(seconds):\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    milliseconds = int((seconds % 1) * 1000)\n",
    "    return f\"{int(hours):02}:{int(minutes):02}:{int(seconds):02}.{milliseconds:03}\"\n",
    "\n",
    "def save_transcripts(segments, output_dir, relative_path, pseudonym_dict=None):\n",
    "    if pseudonym_dict:\n",
    "        for segment in segments:\n",
    "            segment['text'] = anonymize_text(segment['text'], pseudonym_dict)\n",
    "    for i, segment in enumerate(segments):\n",
    "        segment['sentence_number'] = i + 1\n",
    "    df = pd.DataFrame(segments)\n",
    "    df['text'] = df['text'].apply(lambda x: x.lstrip())\n",
    "    cols = ['sentence_number'] + [col for col in df.columns if col != 'sentence_number']\n",
    "    df = df[cols]\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    base_filename = os.path.splitext(os.path.basename(relative_path))[0]\n",
    "    csv_path = os.path.join(output_dir, f\"{base_filename}_transcription.csv\")\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    with open(os.path.join(output_dir, f\"{base_filename}_transcription.txt\"), 'w', encoding='utf-8') as f:\n",
    "        for segment in segments:\n",
    "            f.write(f\"{segment['text'].strip()}\\n\")\n",
    "\n",
    "    json_path = os.path.join(output_dir, f\"{base_filename}_transcription.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(segments, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    vtt = webvtt.WebVTT()\n",
    "    for segment in segments:\n",
    "        caption = webvtt.Caption()\n",
    "        caption.start = format_vtt_timestamp(segment['start'])\n",
    "        caption.end = format_vtt_timestamp(segment['end'])\n",
    "        caption.lines = [f\"{segment['sentence_number']}: {segment['text'].strip()}\"]\n",
    "        vtt.captions.append(caption)\n",
    "    vtt.save(os.path.join(output_dir, f\"{base_filename}_transcription.vtt\"))\n",
    "\n",
    "def process_audio_file(audio_file, base_output_dir, relative_path, pseudonym_dict=None):\n",
    "    try:\n",
    "        print(f\"Processing {audio_file}...\")\n",
    "        audio = whisperx.load_audio(audio_file)\n",
    "        model = whisperx.load_model(whisperx_model, device, compute_type=compute_type)\n",
    "        result = model.transcribe(audio, batch_size=batch_size, language=language, task=task)\n",
    "        del model; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "        model_a, metadata = whisperx.load_align_model(language_code=language, device=device)\n",
    "        result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "        del model_a; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "        # Correct way to load diarization model in recent whisperx\n",
    "        diarize_model = whisperx.DiarizationPipeline(use_auth_token=hf_token, device=device)\n",
    "        diarize_segments = diarize_model(audio)\n",
    "        result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "\n",
    "        output_dir = os.path.join(base_output_dir, os.path.dirname(relative_path))\n",
    "        save_transcripts(result[\"segments\"], output_dir, relative_path, pseudonym_dict)\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error processing {audio_file}:\\n{traceback.format_exc()}\")\n",
    "\n",
    "def main():\n",
    "    # Initialize Tkinter\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "    \n",
    "    # Bring the root window to the front\n",
    "    root.attributes('-topmost', True)\n",
    "\n",
    "    # Popup for input folder\n",
    "    input_folder = filedialog.askdirectory(title=\"Select Folder Containing Audio/Video Files\")\n",
    "    if not input_folder:\n",
    "        print(\"No folder selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Popup for output folder\n",
    "    output_folder = filedialog.askdirectory(title=\"Select Folder to Save Transcriptions\")\n",
    "    if not output_folder:\n",
    "        print(\"No output folder selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Ask if a pseudonyms.csv file will be used\n",
    "    use_pseudonyms = messagebox.askyesno(\"Pseudonyms\", \"Will you use a pseudonyms.csv file for to anonymize the transcripts?\")\n",
    "    pseudonym_dict = None\n",
    "\n",
    "    if use_pseudonyms:\n",
    "        pseudonyms_file = filedialog.askopenfilename(\n",
    "            title=\"Select Pseudonyms CSV File\",\n",
    "            filetypes=[(\"CSV files\", \"*.csv\")]\n",
    "        )\n",
    "        if not pseudonyms_file:\n",
    "            print(\"No pseudonyms file selected. Continuing without pseudonymization.\")\n",
    "        else:\n",
    "            # Load the pseudonyms file\n",
    "            pseudonyms_df = pd.read_csv(pseudonyms_file)\n",
    "            pseudonym_dict = dict(zip(pseudonyms_df['name'], pseudonyms_df['pseudonym']))\n",
    "            print(f\"Pseudonyms loaded from {pseudonyms_file}.\")\n",
    "\n",
    "    # Find and process audio files\n",
    "    audio_files = find_audio_files(input_folder, extensions)\n",
    "    print(f\"Found {len(audio_files)} files to process.\")\n",
    "\n",
    "    for audio_file in audio_files:\n",
    "        relative_path = os.path.relpath(audio_file, input_folder)\n",
    "        process_audio_file(audio_file, output_folder, relative_path, pseudonym_dict)\n",
    "        print(f\"Processed {audio_file}\")\n",
    "\n",
    "    print(\"All files processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperxtranscription-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
